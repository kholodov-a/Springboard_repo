{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "from myutils import seed_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1812\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'BATCH_SIZE': 32,               # Batch size\n",
    "    'INITIAL_LR': 0.001,            # Initial Learning rate\n",
    "    'LR_DECOY': 0.9,                # Learning rate decay factor applies each epoch\n",
    "    'NUM_EPOCHS': 10,               # Number of epochs for model training\n",
    "    'DATA_DIR': '../Data/raw',      # Folder with images in initial format\n",
    "    'IMAGE_SIZE': 242,              # Target image size (width and height) for preprocessing\n",
    "    'NUM_CLASSES': 6,               # Number of classes\n",
    "    'TEST_SPLIT_RATIO': 0.2,        # Share of testing data\n",
    "    'VALID_SPLIT_RATIO': 0.1,       # Share of validation data\n",
    "    'NORMALIZE': True,              # Images normalization\n",
    "    'DEBUG': True,                  # Whether to output debug info or not\n",
    "    'NUMBER_OF_TRIALS': 1,          # Number of trials to calculate the mean time per image\n",
    "    'NUM_WORKER': 8,                # Number of workers for DataLoader\n",
    "    'PREFETCH_FACTOR': 2,           # Prefetch factor for DataLoader\n",
    "}\n",
    "\n",
    "# ANSI escape codes\n",
    "RED = '\\033[91m'\n",
    "GREEN = '\\033[92m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "# Logging configuration for all modules, including Python's pachages \n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.WARNING)\n",
    "root_handler = logging.StreamHandler()\n",
    "root_handler.setFormatter(logging.Formatter('%(asctime)s - ROOT - %(levelname)s - %(message)s'))\n",
    "root_logger.addHandler(root_handler)\n",
    "\n",
    "# Logging configuration for my module\n",
    "my_module_logger = logging.getLogger(__name__)\n",
    "if config['DEBUG'] == True:\n",
    "    my_module_logger.setLevel(logging.DEBUG)\n",
    "else:\n",
    "    my_module_logger.setLevel(logging.INFO)\n",
    "my_module_logger.propagate = False\n",
    "\n",
    "# Handler for my module \n",
    "my_module_handler = logging.StreamHandler()\n",
    "my_module_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "my_module_logger.addHandler(my_module_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seeds(seed = 1812):\n",
    "    # Python, NumPy, and PyTorch seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # For M1 GPU (MPS backend)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    \n",
    "    # Ensure deterministic algorithms (if supported)\n",
    "    torch.backends.cudnn.deterministic = True  # Affects CPU/CUDA/MPS indirectly\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    # torch.backends.mps.deterministic = True\n",
    "\n",
    "# Detecting the type of avalable device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesData:\n",
    "    def __init__(self, data_dir: str = config['DATA_DIR'], \n",
    "                 test_split: float = config['TEST_SPLIT_RATIO'], \n",
    "                 validation_split: float = config['VALID_SPLIT_RATIO']):\n",
    "        self.data_dir = data_dir\n",
    "        self.test_split = test_split\n",
    "        self.valid_split = validation_split\n",
    "\n",
    "        self.train_dataset, self.valid_dataset, self.test_dataset = self.get_datasets(data_dir)\n",
    "        self.train_loader, self.valid_loader, self.test_loader = self.get_dataloaders(self.train_dataset, \n",
    "                                                                                      self.valid_dataset, \n",
    "                                                                                      self.test_dataset\n",
    "                                                                                      )\n",
    "\n",
    "    # Composing transformers for training, validation and testing \n",
    "    def get_transforms(self, train: bool = False):\n",
    "        # The common part for all kind of transformers\n",
    "        common_transforms = [transforms.Resize((config['IMAGE_SIZE'], config['IMAGE_SIZE']))]\n",
    "\n",
    "        # Specificly for training - consist augmentation\n",
    "        \n",
    "        if train:\n",
    "            common_transforms += [\n",
    "                transforms.RandomHorizontalFlip(p = 0.5),     # Horizontal Flip\n",
    "                transforms.RandomVerticalFlip(p = 0.5),       # Vertical Flip\n",
    "                transforms.RandomAffine(                    \n",
    "                    degrees = 36,                             # Rotation\n",
    "                    translate = (0.2, 0.2),                   # Move\n",
    "                    scale = (0.7, 1.3),                       # Zoom\n",
    "                    interpolation = InterpolationMode.NEAREST # How to interpolate - as neighbor pixels\n",
    "                ),\n",
    "                transforms.ColorJitter(brightness = 0.2, contrast = 0.2)    # Brightness and contrast\n",
    "            ]\n",
    "        # Convertion to tensor for all kind of transformers \n",
    "        common_transforms.append(transforms.ToTensor())\n",
    "        # Normalization \n",
    "        if config['NORMALIZE']:\n",
    "            common_transforms.append(\n",
    "                transforms.Normalize(mean = [0.485, 0.456, 0.406],  # Normalization parameters the same as for ImageNet\n",
    "                                     std = [0.229, 0.224, 0.225])\n",
    "            )\n",
    "        return transforms.Compose(common_transforms)\n",
    "\n",
    "\n",
    "    def get_datasets(self, data_dir: str):\n",
    "        train_transforms = self.get_transforms(train = True)\n",
    "        valid_transforms = self.get_transforms(train = False)\n",
    "        test_transforms  = self.get_transforms(train = False)\n",
    "        \n",
    "        train_dataset = datasets.ImageFolder(root = data_dir, transform = train_transforms)\n",
    "        valid_dataset = datasets.ImageFolder(root = data_dir, transform = valid_transforms)\n",
    "        test_dataset  = datasets.ImageFolder(root = data_dir, transform = test_transforms)\n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    def get_dataloaders(self, train_dataset, valid_dataset, test_dataset):\n",
    "        num_workers = config['NUM_WORKER']\n",
    "        prefetch_factor = config['PREFETCH_FACTOR']\n",
    "        my_module_logger.debug(f'{GREEN}Number of workers: {num_workers:d}, '\n",
    "                               f'Prefetch factor: {prefetch_factor:d}, '\n",
    "                               f'Batch size: {config['BATCH_SIZE']:d}{RESET}')\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(1812)\n",
    "\n",
    "        dataset_size = len(train_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        np.random.seed(1812)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        dataset_size = 200\n",
    "        indices = indices[:dataset_size]\n",
    "\n",
    "        test_split = int(np.floor(config['TEST_SPLIT_RATIO'] * dataset_size))\n",
    "        valid_split = int(np.floor(config['VALID_SPLIT_RATIO'] * dataset_size))\n",
    "\n",
    "        test_indices = indices[:test_split]\n",
    "        train_val_indices = indices[test_split:]\n",
    "        valid_indices = train_val_indices[:valid_split]\n",
    "        train_indices = train_val_indices[valid_split:]\n",
    "\n",
    "        my_module_logger.debug(f'Training dataset indices: {min(train_indices):d}-{max(train_indices):d}, ' \n",
    "                               f'Number of instances: {len(train_indices)}')\n",
    "        my_module_logger.debug(f'Validation dataset indices: {min(valid_indices):d}-{max(valid_indices):d}, '\n",
    "                               f'Number of instances: {len(valid_indices)}')\n",
    "        my_module_logger.debug(f'Test dataset indices: {min(test_indices):d}-{max(test_indices):d}, '\n",
    "                               f'Number of instances: {len(test_indices)}')                \n",
    "\n",
    "        my_module_logger.debug(f'Nun_workers: {num_workers}, prefetch_factor: {prefetch_factor}')\n",
    "        train_loader = DataLoader(Subset(train_dataset, train_indices),\n",
    "                                batch_size = config['BATCH_SIZE'], shuffle = True,\n",
    "                                num_workers = num_workers,              # Number of subprocesses for data loading\n",
    "                                # pin_memory = True,                      # Pinned memory for faster transfers to GPU,\n",
    "                                prefetch_factor = prefetch_factor,      # Number of batches to prefetch per worker\n",
    "                                persistent_workers = False,\n",
    "                                generator = g,\n",
    "                                worker_init_fn = seed_worker\n",
    "                                )\n",
    "        valid_loader = DataLoader(Subset(valid_dataset, valid_indices),\n",
    "                                batch_size = config['BATCH_SIZE'], shuffle = False,\n",
    "                                num_workers = num_workers,         \n",
    "                                # pin_memory = True,       \n",
    "                                prefetch_factor = prefetch_factor,\n",
    "                                persistent_workers = False, \n",
    "                                generator = g,\n",
    "                                worker_init_fn = seed_worker\n",
    "                                )\n",
    "        test_loader  = DataLoader(Subset(test_dataset, test_indices),\n",
    "                                batch_size = config['BATCH_SIZE'], shuffle = False, \n",
    "                                num_workers = num_workers,         \n",
    "                                # pin_memory = True,       \n",
    "                                prefetch_factor = prefetch_factor,\n",
    "                                persistent_workers = False,\n",
    "                                generator = g,\n",
    "                                worker_init_fn = seed_worker\n",
    "                                )\n",
    "        return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_metrics, val_metrics, ylabel: str, title: str):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_metrics, label='Training')\n",
    "    plt.plot(val_metrics, label='Validation')\n",
    "    if ylabel == 'Accuracy':\n",
    "        plt.ylim([0, 100])\n",
    "    else:\n",
    "        plt.ylim([0, 1])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def denormalize(image, mean, std):\n",
    "    mean = torch.tensor(mean).reshape(-1, 1, 1)\n",
    "    std = torch.tensor(std).reshape(-1, 1, 1)\n",
    "    image = image * std + mean\n",
    "    return image\n",
    "\n",
    "def plot_samples_from_loader(loader, num:int):\n",
    "    batch = iter(loader)\n",
    "    first_sixteen = next(batch)\n",
    "\n",
    "    # Assuming first_sixteen[0] is a batch of images with shape (32, 3, 242, 242)\n",
    "    # Select a subset of images, for example, the first 16 images\n",
    "    images = first_sixteen[0][:16]              # Shape: (16, 3, 242, 242)\n",
    "    denorm_images = denormalize(images, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    # Create a grid of images\n",
    "    grid_img = torchvision.utils.make_grid(denorm_images, nrow = 4)  # nrow defines the number of images in each row\n",
    "\n",
    "    # The grid image will have shape (3, H, W), so we need to permute dimensions\n",
    "    grid_img = grid_img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_samples(images, num:int):\n",
    "    # Assuming first_sixteen[0] is a batch of images with shape (32, 3, 242, 242)\n",
    "    # Select a subset of images, for example, the first 16 images\n",
    "    images = images[:num]              # Shape: (32, 3, 242, 242)\n",
    "    images = images.to(torch.device('cpu'))\n",
    "    denorm_images = denormalize(images, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    # Create a grid of images\n",
    "    grid_img = torchvision.utils.make_grid(denorm_images, nrow = 4)  # nrow defines the number of images in each row\n",
    "\n",
    "    # The grid image will have shape (3, H, W), so we need to permute dimensions\n",
    "    grid_img = grid_img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, (num // 4) * 3))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes: int):\n",
    "    model = torchvision.models.convnext_base(\n",
    "        weights = torchvision.models.ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
    "    num_features = model.classifier[2].in_features\n",
    "    model.classifier[2] = nn.Linear(num_features, num_classes)\n",
    "    # Freeze feature extractor\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batches_debuging_info = []\n",
    "    batch = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        checksums_for_debuging = defaultdict(dict)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        weights = 7 * torch.arange(1, images.shape[0] + 1).view(images.shape[0], 1, 1, 1)    # to account the possible shuffle of images inside the batch\n",
    "        weights = weights.to(get_device())\n",
    "        images_check_sum = (images *  weights).sum()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        my_module_logger.debug(f'Images chechsum: {images_check_sum:12,.0f}, '        # Checksum for all images in the batch to compare with other iterations\n",
    "                               f'Outputs checksum: {outputs.sum():8.4f}, '               # checksum for outputs to compare with other iterations\n",
    "                               f'Loss: {loss:2.6f}')                                     # The loss to compare with other iterations \n",
    "\n",
    "        # plot_samples(images, 4)                               # Display the first four images of the batch to check that they are the same on the each iteration\n",
    "\n",
    "        checksums_for_debuging['Batch'] = batch\n",
    "        checksums_for_debuging['Images_check'] = images_check_sum.item()\n",
    "        checksums_for_debuging['Outputs_check'] = outputs.sum().item()\n",
    "        checksums_for_debuging['Loss'] = loss.item()\n",
    "        batches_debuging_info.append(checksums_for_debuging)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        batch += 1\n",
    "    return running_loss / total, 100 * correct / total, batches_debuging_info\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / total, 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grid(parameters: list):\n",
    "    combinations = []\n",
    "    for grid in parameters:\n",
    "        # Converting all values to list\n",
    "        normalized_grid = {key: value if isinstance(value, list) else [value]\n",
    "                           for key, value in grid.items()}\n",
    "        # Exctract keys and respective values\n",
    "        keys = list(normalized_grid.keys())\n",
    "        values_lists = [normalized_grid[key] for key in keys]\n",
    "\n",
    "        # Generating all possible combinations\n",
    "        combinations.extend([dict(zip(keys, combination)) for combination in itertools.product(*values_lists)])\n",
    "        \n",
    "    return combinations\n",
    "\n",
    "def change_config(combination: dict):\n",
    "     for key, value in combination.items():\n",
    "         config[key] = value\n",
    "\n",
    "def log_results(file_name, config, test_loss, test_acc, elapsed_time):\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for key, value in config.items():\n",
    "        results[key] = value\n",
    "    results['test_loss'] = test_loss\n",
    "    results['test_acc'] = test_acc\n",
    "    results['elapsed_time'] = elapsed_time /60\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict([results])\n",
    "    \n",
    "    header = not os.path.exists(file_name)\n",
    "    results_df.to_csv(file_name, mode = 'a', header = header, index = False)\n",
    "\n",
    "def flatten_defaultdict(data):\n",
    "    rows = []\n",
    "    for key, value_list in data.items():\n",
    "        for _, item in enumerate(value_list):\n",
    "            row = {'Iteration': key}\n",
    "            row.update(item)\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    reset_seeds(1812)\n",
    "\n",
    "    device = get_device()\n",
    "    my_module_logger.debug(f'Using device: {device}')\n",
    "    my_module_logger.debug(f'Config: {config}')\n",
    "\n",
    "    data = ImagesData()\n",
    "\n",
    "    model = build_model(config['NUM_CLASSES'])\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = config['INITIAL_LR'])\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = config['LR_DECOY'])\n",
    "\n",
    "    epoch_train_loss, epoch_train_acc = [], []\n",
    "    epoch_val_loss, epoch_val_acc = [], []\n",
    "    debug_info_list = []\n",
    "\n",
    "    for epoch in range(config['NUM_EPOCHS']):\n",
    "        torch.cuda.empty_cache()\n",
    "        start_time = time.time()\n",
    "   \n",
    "        train_loss, train_acc, debug_info = train_one_epoch(model, data.train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, data.valid_loader, criterion, device)\n",
    "\n",
    "        # Adding debuging infor for the epoch\n",
    "        debug_info = [{'Epoch': epoch + 1, **d} for d in debug_info]\n",
    "        debug_info_list.extend(debug_info)\n",
    "\n",
    "        epoch_train_loss.append(train_loss)\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_val_loss.append(val_loss)\n",
    "        epoch_val_acc.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        my_module_logger.debug(f'Epoch [{epoch + 1:2d}/{config['NUM_EPOCHS']}]: '\n",
    "                    f'Train Loss: {RED}{train_loss:.4f}{RESET}, Train Acc: {train_acc:.2f}%, '\n",
    "                    f'Val Loss: {RED}{val_loss:.4f}{RESET}, Val Acc: {val_acc:.2f}%, '\n",
    "                    f'Time: {elapsed:3.0f} sec')\n",
    "\n",
    "    # Save the model\n",
    "    # torch.save(model, '../Models/convNeXt_pytorch_base_v5_BS' + \n",
    "    #            str(config['BATCH_SIZE']) + '_LR' + str(config['INITIAL_LR']))\n",
    "\n",
    "    # Evaluate on test set with timing measurements\n",
    "    processing_times = []\n",
    "\n",
    "    for i in range(config['NUMBER_OF_TRIALS']):\n",
    "        start_trial = time.time()\n",
    "        test_loss, test_acc = evaluate(model, data.test_loader, criterion, device)\n",
    "        elapsed_trial = time.time() - start_trial\n",
    "\n",
    "        # Assuming total images is sum of batch sizes:\n",
    "        total_images = len(data.test_dataset)\n",
    "        avg_time_per_image = elapsed_trial / total_images\n",
    "        \n",
    "        my_module_logger.debug(f'Trial: {i + 1}, '\n",
    "                               f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, '\n",
    "                               f'Avg Time/Image: {avg_time_per_image:.3f} sec')\n",
    "        processing_times.append(avg_time_per_image)\n",
    "\n",
    "    my_module_logger.info(f'Final Test: Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%')\n",
    "    my_module_logger.info(f'Mean processing time per image: {np.mean(processing_times):.3f} sec')\n",
    "\n",
    "    # plot_metrics(epoch_train_acc, epoch_val_acc, 'Accuracy', 'Training and Validation Accuracy')\n",
    "    # plot_metrics(epoch_train_loss, epoch_val_loss, 'Loss', 'Training and Validation Loss')\n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    del criterion\n",
    "    del scheduler \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_loss, test_acc, debug_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 19:24:38,118 - DEBUG - \u001b[92mIteration 1\u001b[0m\n",
      "2025-02-05 19:24:38,139 - DEBUG - Using device: mps\n",
      "2025-02-05 19:24:38,139 - DEBUG - Config: {'BATCH_SIZE': 32, 'INITIAL_LR': 0.001, 'LR_DECOY': 0.9, 'NUM_EPOCHS': 10, 'DATA_DIR': '../Data/raw', 'IMAGE_SIZE': 242, 'NUM_CLASSES': 6, 'TEST_SPLIT_RATIO': 0.2, 'VALID_SPLIT_RATIO': 0.1, 'NORMALIZE': True, 'DEBUG': True, 'NUMBER_OF_TRIALS': 1, 'NUM_WORKER': 8, 'PREFETCH_FACTOR': 2}\n",
      "2025-02-05 19:24:38,151 - DEBUG - \u001b[92mNumber of workers: 8, Prefetch factor: 2, Batch size: 32\u001b[0m\n",
      "2025-02-05 19:24:38,152 - DEBUG - Training dataset indices: 4-2493, Number of instances: 140\n",
      "2025-02-05 19:24:38,152 - DEBUG - Validation dataset indices: 115-2484, Number of instances: 20\n",
      "2025-02-05 19:24:38,152 - DEBUG - Test dataset indices: 72-2526, Number of instances: 40\n",
      "2025-02-05 19:24:38,152 - DEBUG - Nun_workers: 8, prefetch_factor: 2\n",
      "2025-02-05 19:24:50,463 - DEBUG - Images chechsum:   96,798,416, Outputs checksum:  -5.7422, Loss: 1.778519\n",
      "2025-02-05 19:24:51,881 - DEBUG - Images chechsum:  108,718,832, Outputs checksum:  -1.6453, Loss: 1.741768\n",
      "2025-02-05 19:24:53,167 - DEBUG - Images chechsum:   77,164,784, Outputs checksum:  -8.3567, Loss: 1.693612\n",
      "2025-02-05 19:24:54,454 - DEBUG - Images chechsum:   56,238,016, Outputs checksum: -11.7791, Loss: 1.635712\n",
      "2025-02-05 19:24:54,995 - DEBUG - Images chechsum:  -21,610,722, Outputs checksum:  -5.0679, Loss: 1.621746\n",
      "2025-02-05 19:25:32,013 - DEBUG - Epoch [ 1/10]: Train Loss: \u001b[91m1.7046\u001b[0m, Train Acc: 28.57%, Val Loss: \u001b[91m1.6347\u001b[0m, Val Acc: 35.00%, Time:  52 sec\n",
      "2025-02-05 19:25:41,185 - DEBUG - Images chechsum:   84,747,416, Outputs checksum: -23.4933, Loss: 1.471361\n",
      "2025-02-05 19:25:42,493 - DEBUG - Images chechsum:  119,699,776, Outputs checksum: -16.5543, Loss: 1.390118\n",
      "2025-02-05 19:25:43,787 - DEBUG - Images chechsum:  174,519,712, Outputs checksum:  -9.8078, Loss: 1.434784\n",
      "2025-02-05 19:25:45,079 - DEBUG - Images chechsum:   44,154,360, Outputs checksum: -13.3197, Loss: 1.390435\n",
      "2025-02-05 19:25:45,584 - DEBUG - Images chechsum:   22,315,032, Outputs checksum:  -7.1024, Loss: 1.215767\n",
      "2025-02-05 19:26:22,155 - DEBUG - Epoch [ 2/10]: Train Loss: \u001b[91m1.4040\u001b[0m, Train Acc: 72.14%, Val Loss: \u001b[91m1.4691\u001b[0m, Val Acc: 60.00%, Time:  50 sec\n",
      "2025-02-05 19:26:31,761 - DEBUG - Images chechsum:   57,769,408, Outputs checksum: -24.8159, Loss: 1.245508\n",
      "2025-02-05 19:26:33,068 - DEBUG - Images chechsum:   15,081,962, Outputs checksum: -28.7897, Loss: 1.219098\n",
      "2025-02-05 19:26:34,358 - DEBUG - Images chechsum:   44,012,520, Outputs checksum: -21.3269, Loss: 1.157268\n",
      "2025-02-05 19:26:35,642 - DEBUG - Images chechsum:  105,083,656, Outputs checksum: -20.6655, Loss: 1.282876\n",
      "2025-02-05 19:26:36,138 - DEBUG - Images chechsum:   44,016,852, Outputs checksum: -10.1992, Loss: 1.033710\n",
      "2025-02-05 19:27:12,942 - DEBUG - Epoch [ 3/10]: Train Loss: \u001b[91m1.2097\u001b[0m, Train Acc: 74.29%, Val Loss: \u001b[91m1.3551\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:27:22,503 - DEBUG - Images chechsum:   -4,317,456, Outputs checksum: -22.2980, Loss: 1.119152\n",
      "2025-02-05 19:27:23,837 - DEBUG - Images chechsum:  108,442,728, Outputs checksum: -23.3553, Loss: 1.145063\n",
      "2025-02-05 19:27:25,141 - DEBUG - Images chechsum:   97,282,792, Outputs checksum: -25.3762, Loss: 0.916697\n",
      "2025-02-05 19:27:26,442 - DEBUG - Images chechsum:   75,521,968, Outputs checksum: -25.5099, Loss: 1.117010\n",
      "2025-02-05 19:27:26,945 - DEBUG - Images chechsum:   24,692,338, Outputs checksum: -16.8456, Loss: 0.906860\n",
      "2025-02-05 19:28:03,611 - DEBUG - Epoch [ 4/10]: Train Loss: \u001b[91m1.0601\u001b[0m, Train Acc: 80.00%, Val Loss: \u001b[91m1.2726\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:28:13,334 - DEBUG - Images chechsum:  172,908,192, Outputs checksum: -21.0944, Loss: 0.970764\n",
      "2025-02-05 19:28:14,640 - DEBUG - Images chechsum:  113,435,824, Outputs checksum: -34.8822, Loss: 0.971033\n",
      "2025-02-05 19:28:15,936 - DEBUG - Images chechsum:  138,261,824, Outputs checksum: -23.0489, Loss: 0.967060\n",
      "2025-02-05 19:28:17,219 - DEBUG - Images chechsum:  200,831,840, Outputs checksum: -32.9539, Loss: 0.979024\n",
      "2025-02-05 19:28:17,718 - DEBUG - Images chechsum:   22,348,710, Outputs checksum: -10.1977, Loss: 1.040490\n",
      "2025-02-05 19:28:54,234 - DEBUG - Epoch [ 5/10]: Train Loss: \u001b[91m0.9778\u001b[0m, Train Acc: 78.57%, Val Loss: \u001b[91m1.2107\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:29:04,270 - DEBUG - Images chechsum:   93,153,600, Outputs checksum: -32.1049, Loss: 0.811145\n",
      "2025-02-05 19:29:05,597 - DEBUG - Images chechsum:  142,920,496, Outputs checksum: -28.3668, Loss: 0.920377\n",
      "2025-02-05 19:29:06,887 - DEBUG - Images chechsum:  -10,750,986, Outputs checksum: -25.5574, Loss: 1.071934\n",
      "2025-02-05 19:29:08,184 - DEBUG - Images chechsum:   93,637,656, Outputs checksum: -27.0082, Loss: 0.679622\n",
      "2025-02-05 19:29:08,676 - DEBUG - Images chechsum:   16,645,909, Outputs checksum: -12.6816, Loss: 0.808829\n",
      "2025-02-05 19:29:45,273 - DEBUG - Epoch [ 6/10]: Train Loss: \u001b[91m0.8655\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.1605\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:29:56,688 - DEBUG - Images chechsum:  256,731,712, Outputs checksum: -38.1565, Loss: 0.762962\n",
      "2025-02-05 19:29:58,206 - DEBUG - Images chechsum:  -16,570,136, Outputs checksum: -32.3899, Loss: 0.794405\n",
      "2025-02-05 19:29:59,576 - DEBUG - Images chechsum:   98,761,696, Outputs checksum: -32.9083, Loss: 0.892960\n",
      "2025-02-05 19:30:00,929 - DEBUG - Images chechsum:   33,314,864, Outputs checksum: -32.0120, Loss: 0.731299\n",
      "2025-02-05 19:30:01,530 - DEBUG - Images chechsum:   -6,805,710, Outputs checksum: -10.4980, Loss: 0.926953\n",
      "2025-02-05 19:30:38,969 - DEBUG - Epoch [ 7/10]: Train Loss: \u001b[91m0.8067\u001b[0m, Train Acc: 86.43%, Val Loss: \u001b[91m1.1221\u001b[0m, Val Acc: 65.00%, Time:  54 sec\n",
      "2025-02-05 19:30:48,965 - DEBUG - Images chechsum:   10,231,496, Outputs checksum: -36.1360, Loss: 0.728612\n",
      "2025-02-05 19:30:50,447 - DEBUG - Images chechsum:  162,842,656, Outputs checksum: -31.5580, Loss: 0.848911\n",
      "2025-02-05 19:30:51,928 - DEBUG - Images chechsum:   56,932,336, Outputs checksum: -37.6425, Loss: 0.774005\n",
      "2025-02-05 19:30:53,387 - DEBUG - Images chechsum:   45,655,088, Outputs checksum: -26.7111, Loss: 0.756241\n",
      "2025-02-05 19:30:53,958 - DEBUG - Images chechsum:   11,998,537, Outputs checksum:  -8.6436, Loss: 0.655916\n",
      "2025-02-05 19:31:30,792 - DEBUG - Epoch [ 8/10]: Train Loss: \u001b[91m0.7666\u001b[0m, Train Acc: 84.29%, Val Loss: \u001b[91m1.0896\u001b[0m, Val Acc: 65.00%, Time:  52 sec\n",
      "2025-02-05 19:31:40,852 - DEBUG - Images chechsum:   12,704,690, Outputs checksum: -27.6101, Loss: 0.778350\n",
      "2025-02-05 19:31:42,431 - DEBUG - Images chechsum:  190,852,528, Outputs checksum: -38.8268, Loss: 0.681196\n",
      "2025-02-05 19:31:43,936 - DEBUG - Images chechsum:   63,389,208, Outputs checksum: -34.6175, Loss: 0.654141\n",
      "2025-02-05 19:31:45,409 - DEBUG - Images chechsum:   48,731,196, Outputs checksum: -29.0039, Loss: 0.696909\n",
      "2025-02-05 19:31:45,970 - DEBUG - Images chechsum:   23,594,842, Outputs checksum: -13.8746, Loss: 0.668102\n",
      "2025-02-05 19:32:22,835 - DEBUG - Epoch [ 9/10]: Train Loss: \u001b[91m0.6997\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.0642\u001b[0m, Val Acc: 65.00%, Time:  52 sec\n",
      "2025-02-05 19:32:32,524 - DEBUG - Images chechsum:   59,281,200, Outputs checksum: -35.1034, Loss: 0.713422\n",
      "2025-02-05 19:32:33,846 - DEBUG - Images chechsum:   76,382,136, Outputs checksum: -40.7213, Loss: 0.590486\n",
      "2025-02-05 19:32:35,154 - DEBUG - Images chechsum:  -51,478,012, Outputs checksum: -29.8884, Loss: 0.683046\n",
      "2025-02-05 19:32:36,459 - DEBUG - Images chechsum:   75,953,576, Outputs checksum: -29.2805, Loss: 0.769206\n",
      "2025-02-05 19:32:36,971 - DEBUG - Images chechsum:   14,413,874, Outputs checksum:  -8.5973, Loss: 0.674232\n",
      "2025-02-05 19:33:13,663 - DEBUG - Epoch [10/10]: Train Loss: \u001b[91m0.6878\u001b[0m, Train Acc: 91.43%, Val Loss: \u001b[91m1.0433\u001b[0m, Val Acc: 70.00%, Time:  51 sec\n",
      "2025-02-05 19:33:33,186 - DEBUG - Trial: 1, Test Loss: 0.7794, Test Acc: 85.00%, Avg Time/Image: 0.008 sec\n",
      "2025-02-05 19:33:33,190 - INFO - Final Test: Loss: 0.7794, Acc: 85.00%\n",
      "2025-02-05 19:33:33,200 - INFO - Mean processing time per image: 0.008 sec\n",
      "2025-02-05 19:33:33,211 - DEBUG - Total cycle elapsed time:  8 min 55 sec\n",
      "2025-02-05 19:33:33,284 - DEBUG - \u001b[92mIteration 2\u001b[0m\n",
      "2025-02-05 19:33:33,298 - DEBUG - Using device: mps\n",
      "2025-02-05 19:33:33,298 - DEBUG - Config: {'BATCH_SIZE': 32, 'INITIAL_LR': 0.001, 'LR_DECOY': 0.9, 'NUM_EPOCHS': 10, 'DATA_DIR': '../Data/raw', 'IMAGE_SIZE': 242, 'NUM_CLASSES': 6, 'TEST_SPLIT_RATIO': 0.2, 'VALID_SPLIT_RATIO': 0.1, 'NORMALIZE': True, 'DEBUG': True, 'NUMBER_OF_TRIALS': 1, 'NUM_WORKER': 8, 'PREFETCH_FACTOR': 2}\n",
      "2025-02-05 19:33:33,317 - DEBUG - \u001b[92mNumber of workers: 8, Prefetch factor: 2, Batch size: 32\u001b[0m\n",
      "2025-02-05 19:33:33,318 - DEBUG - Training dataset indices: 4-2493, Number of instances: 140\n",
      "2025-02-05 19:33:33,319 - DEBUG - Validation dataset indices: 115-2484, Number of instances: 20\n",
      "2025-02-05 19:33:33,319 - DEBUG - Test dataset indices: 72-2526, Number of instances: 40\n",
      "2025-02-05 19:33:33,320 - DEBUG - Nun_workers: 8, prefetch_factor: 2\n",
      "2025-02-05 19:33:45,679 - DEBUG - Images chechsum:   96,798,416, Outputs checksum:  -5.7422, Loss: 1.778519\n",
      "2025-02-05 19:33:47,005 - DEBUG - Images chechsum:  108,718,832, Outputs checksum:  -1.7106, Loss: 1.741565\n",
      "2025-02-05 19:33:48,312 - DEBUG - Images chechsum:   77,164,784, Outputs checksum:  -8.2968, Loss: 1.693286\n",
      "2025-02-05 19:33:49,604 - DEBUG - Images chechsum:   56,238,016, Outputs checksum: -11.6934, Loss: 1.635638\n",
      "2025-02-05 19:33:50,100 - DEBUG - Images chechsum:  -21,610,722, Outputs checksum:  -4.9956, Loss: 1.621296\n",
      "2025-02-05 19:34:26,628 - DEBUG - Epoch [ 1/10]: Train Loss: \u001b[91m1.7045\u001b[0m, Train Acc: 27.86%, Val Loss: \u001b[91m1.6363\u001b[0m, Val Acc: 35.00%, Time:  51 sec\n",
      "2025-02-05 19:34:35,952 - DEBUG - Images chechsum:   84,747,416, Outputs checksum: -23.1232, Loss: 1.474313\n",
      "2025-02-05 19:34:37,270 - DEBUG - Images chechsum:  119,699,776, Outputs checksum: -16.2695, Loss: 1.391165\n",
      "2025-02-05 19:34:38,566 - DEBUG - Images chechsum:  174,519,712, Outputs checksum:  -9.6048, Loss: 1.436953\n",
      "2025-02-05 19:34:39,853 - DEBUG - Images chechsum:   44,154,360, Outputs checksum: -13.1407, Loss: 1.392747\n",
      "2025-02-05 19:34:40,374 - DEBUG - Images chechsum:   22,315,032, Outputs checksum:  -6.9981, Loss: 1.222133\n",
      "2025-02-05 19:35:17,061 - DEBUG - Epoch [ 2/10]: Train Loss: \u001b[91m1.4065\u001b[0m, Train Acc: 72.14%, Val Loss: \u001b[91m1.4744\u001b[0m, Val Acc: 60.00%, Time:  50 sec\n",
      "2025-02-05 19:35:26,630 - DEBUG - Images chechsum:   57,769,408, Outputs checksum: -24.3758, Loss: 1.252894\n",
      "2025-02-05 19:35:27,942 - DEBUG - Images chechsum:   15,081,962, Outputs checksum: -28.3450, Loss: 1.226033\n",
      "2025-02-05 19:35:29,245 - DEBUG - Images chechsum:   44,012,520, Outputs checksum: -21.0162, Loss: 1.164337\n",
      "2025-02-05 19:35:30,545 - DEBUG - Images chechsum:  105,083,656, Outputs checksum: -20.3858, Loss: 1.287519\n",
      "2025-02-05 19:35:31,042 - DEBUG - Images chechsum:   44,016,852, Outputs checksum: -10.0180, Loss: 1.044467\n",
      "2025-02-05 19:36:07,599 - DEBUG - Epoch [ 3/10]: Train Loss: \u001b[91m1.2166\u001b[0m, Train Acc: 74.29%, Val Loss: \u001b[91m1.3640\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:36:17,081 - DEBUG - Images chechsum:   -4,317,456, Outputs checksum: -21.9459, Loss: 1.129237\n",
      "2025-02-05 19:36:18,400 - DEBUG - Images chechsum:  108,442,728, Outputs checksum: -22.9429, Loss: 1.155578\n",
      "2025-02-05 19:36:19,719 - DEBUG - Images chechsum:   97,282,792, Outputs checksum: -25.0838, Loss: 0.930057\n",
      "2025-02-05 19:36:21,033 - DEBUG - Images chechsum:   75,521,968, Outputs checksum: -24.9921, Loss: 1.127588\n",
      "2025-02-05 19:36:21,534 - DEBUG - Images chechsum:   24,692,338, Outputs checksum: -16.4981, Loss: 0.923118\n",
      "2025-02-05 19:36:58,382 - DEBUG - Epoch [ 4/10]: Train Loss: \u001b[91m1.0717\u001b[0m, Train Acc: 80.00%, Val Loss: \u001b[91m1.2847\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:37:09,998 - DEBUG - Images chechsum:  172,908,192, Outputs checksum: -20.8294, Loss: 0.984789\n",
      "2025-02-05 19:37:11,588 - DEBUG - Images chechsum:  113,435,824, Outputs checksum: -34.0388, Loss: 0.984551\n",
      "2025-02-05 19:37:13,116 - DEBUG - Images chechsum:  138,261,824, Outputs checksum: -22.6832, Loss: 0.984116\n",
      "2025-02-05 19:37:14,597 - DEBUG - Images chechsum:  200,831,840, Outputs checksum: -32.3917, Loss: 0.993377\n",
      "2025-02-05 19:37:15,195 - DEBUG - Images chechsum:   22,348,710, Outputs checksum:  -9.9853, Loss: 1.055315\n",
      "2025-02-05 19:37:52,271 - DEBUG - Epoch [ 5/10]: Train Loss: \u001b[91m0.9926\u001b[0m, Train Acc: 79.29%, Val Loss: \u001b[91m1.2253\u001b[0m, Val Acc: 65.00%, Time:  54 sec\n",
      "2025-02-05 19:38:01,808 - DEBUG - Images chechsum:   93,153,600, Outputs checksum: -31.4745, Loss: 0.829567\n",
      "2025-02-05 19:38:03,118 - DEBUG - Images chechsum:  142,920,496, Outputs checksum: -27.8236, Loss: 0.939099\n",
      "2025-02-05 19:38:04,414 - DEBUG - Images chechsum:  -10,750,986, Outputs checksum: -25.1082, Loss: 1.091179\n",
      "2025-02-05 19:38:05,713 - DEBUG - Images chechsum:   93,637,656, Outputs checksum: -26.5739, Loss: 0.699647\n",
      "2025-02-05 19:38:06,212 - DEBUG - Images chechsum:   16,645,909, Outputs checksum: -12.3948, Loss: 0.833734\n",
      "2025-02-05 19:38:42,824 - DEBUG - Epoch [ 6/10]: Train Loss: \u001b[91m0.8851\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.1773\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:38:52,806 - DEBUG - Images chechsum:  256,731,712, Outputs checksum: -37.2278, Loss: 0.785617\n",
      "2025-02-05 19:38:54,133 - DEBUG - Images chechsum:  -16,570,136, Outputs checksum: -31.6605, Loss: 0.811595\n",
      "2025-02-05 19:38:55,453 - DEBUG - Images chechsum:   98,761,696, Outputs checksum: -32.3851, Loss: 0.912876\n",
      "2025-02-05 19:38:56,754 - DEBUG - Images chechsum:   33,314,864, Outputs checksum: -31.3605, Loss: 0.754954\n",
      "2025-02-05 19:38:57,253 - DEBUG - Images chechsum:   -6,805,710, Outputs checksum: -10.2954, Loss: 0.952120\n",
      "2025-02-05 19:39:33,957 - DEBUG - Epoch [ 7/10]: Train Loss: \u001b[91m0.8279\u001b[0m, Train Acc: 86.43%, Val Loss: \u001b[91m1.1408\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:39:43,696 - DEBUG - Images chechsum:   10,231,496, Outputs checksum: -35.4480, Loss: 0.754356\n",
      "2025-02-05 19:39:45,004 - DEBUG - Images chechsum:  162,842,656, Outputs checksum: -30.9745, Loss: 0.874266\n",
      "2025-02-05 19:39:46,300 - DEBUG - Images chechsum:   56,932,336, Outputs checksum: -36.7321, Loss: 0.794633\n",
      "2025-02-05 19:39:47,591 - DEBUG - Images chechsum:   45,655,088, Outputs checksum: -26.1418, Loss: 0.779366\n",
      "2025-02-05 19:39:48,109 - DEBUG - Images chechsum:   11,998,537, Outputs checksum:  -8.5046, Loss: 0.677677\n",
      "2025-02-05 19:40:24,694 - DEBUG - Epoch [ 8/10]: Train Loss: \u001b[91m0.7901\u001b[0m, Train Acc: 83.57%, Val Loss: \u001b[91m1.1096\u001b[0m, Val Acc: 65.00%, Time:  51 sec\n",
      "2025-02-05 19:40:34,392 - DEBUG - Images chechsum:   12,704,690, Outputs checksum: -27.1428, Loss: 0.805418\n",
      "2025-02-05 19:40:35,702 - DEBUG - Images chechsum:  190,852,528, Outputs checksum: -37.7655, Loss: 0.707214\n",
      "2025-02-05 19:40:36,999 - DEBUG - Images chechsum:   63,389,208, Outputs checksum: -33.8308, Loss: 0.679440\n",
      "2025-02-05 19:40:38,302 - DEBUG - Images chechsum:   48,731,196, Outputs checksum: -28.4260, Loss: 0.722415\n",
      "2025-02-05 19:40:38,796 - DEBUG - Images chechsum:   23,594,842, Outputs checksum: -13.4983, Loss: 0.697624\n",
      "2025-02-05 19:41:15,505 - DEBUG - Epoch [ 9/10]: Train Loss: \u001b[91m0.7260\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.0853\u001b[0m, Val Acc: 70.00%, Time:  51 sec\n",
      "2025-02-05 19:41:25,342 - DEBUG - Images chechsum:   59,281,200, Outputs checksum: -34.3334, Loss: 0.740532\n",
      "2025-02-05 19:41:26,668 - DEBUG - Images chechsum:   76,382,136, Outputs checksum: -39.7072, Loss: 0.620774\n",
      "2025-02-05 19:41:27,973 - DEBUG - Images chechsum:  -51,478,012, Outputs checksum: -29.2142, Loss: 0.710264\n",
      "2025-02-05 19:41:29,301 - DEBUG - Images chechsum:   75,953,576, Outputs checksum: -28.6153, Loss: 0.799852\n",
      "2025-02-05 19:41:29,800 - DEBUG - Images chechsum:   14,413,874, Outputs checksum:  -8.4555, Loss: 0.705677\n",
      "2025-02-05 19:42:06,439 - DEBUG - Epoch [10/10]: Train Loss: \u001b[91m0.7168\u001b[0m, Train Acc: 90.71%, Val Loss: \u001b[91m1.0653\u001b[0m, Val Acc: 70.00%, Time:  51 sec\n",
      "2025-02-05 19:42:25,935 - DEBUG - Trial: 1, Test Loss: 0.8105, Test Acc: 85.00%, Avg Time/Image: 0.008 sec\n",
      "2025-02-05 19:42:25,938 - INFO - Final Test: Loss: 0.8105, Acc: 85.00%\n",
      "2025-02-05 19:42:25,942 - INFO - Mean processing time per image: 0.008 sec\n",
      "2025-02-05 19:42:25,952 - DEBUG - Total cycle elapsed time:  8 min 53 sec\n",
      "2025-02-05 19:42:25,983 - DEBUG - \u001b[92mIteration 3\u001b[0m\n",
      "2025-02-05 19:42:25,991 - DEBUG - Using device: mps\n",
      "2025-02-05 19:42:25,991 - DEBUG - Config: {'BATCH_SIZE': 32, 'INITIAL_LR': 0.001, 'LR_DECOY': 0.9, 'NUM_EPOCHS': 10, 'DATA_DIR': '../Data/raw', 'IMAGE_SIZE': 242, 'NUM_CLASSES': 6, 'TEST_SPLIT_RATIO': 0.2, 'VALID_SPLIT_RATIO': 0.1, 'NORMALIZE': True, 'DEBUG': True, 'NUMBER_OF_TRIALS': 1, 'NUM_WORKER': 8, 'PREFETCH_FACTOR': 2}\n",
      "2025-02-05 19:42:26,014 - DEBUG - \u001b[92mNumber of workers: 8, Prefetch factor: 2, Batch size: 32\u001b[0m\n",
      "2025-02-05 19:42:26,015 - DEBUG - Training dataset indices: 4-2493, Number of instances: 140\n",
      "2025-02-05 19:42:26,015 - DEBUG - Validation dataset indices: 115-2484, Number of instances: 20\n",
      "2025-02-05 19:42:26,015 - DEBUG - Test dataset indices: 72-2526, Number of instances: 40\n",
      "2025-02-05 19:42:26,015 - DEBUG - Nun_workers: 8, prefetch_factor: 2\n",
      "2025-02-05 19:42:40,256 - DEBUG - Images chechsum:   96,798,416, Outputs checksum:  -5.7422, Loss: 1.778519\n",
      "2025-02-05 19:42:41,777 - DEBUG - Images chechsum:  108,718,832, Outputs checksum:  -1.7106, Loss: 1.741565\n",
      "2025-02-05 19:42:43,244 - DEBUG - Images chechsum:   77,164,784, Outputs checksum:  -8.2968, Loss: 1.693286\n",
      "2025-02-05 19:42:44,706 - DEBUG - Images chechsum:   56,238,016, Outputs checksum: -11.6934, Loss: 1.635638\n",
      "2025-02-05 19:42:45,278 - DEBUG - Images chechsum:  -21,610,722, Outputs checksum:  -4.9956, Loss: 1.621296\n",
      "2025-02-05 19:43:22,542 - DEBUG - Epoch [ 1/10]: Train Loss: \u001b[91m1.7045\u001b[0m, Train Acc: 27.86%, Val Loss: \u001b[91m1.6363\u001b[0m, Val Acc: 35.00%, Time:  53 sec\n",
      "2025-02-05 19:43:32,494 - DEBUG - Images chechsum:   84,747,416, Outputs checksum: -23.1232, Loss: 1.474313\n",
      "2025-02-05 19:43:34,003 - DEBUG - Images chechsum:  119,699,776, Outputs checksum: -16.2695, Loss: 1.391165\n",
      "2025-02-05 19:43:35,469 - DEBUG - Images chechsum:  174,519,712, Outputs checksum:  -9.6048, Loss: 1.436953\n",
      "2025-02-05 19:43:36,938 - DEBUG - Images chechsum:   44,154,360, Outputs checksum: -13.1407, Loss: 1.392747\n",
      "2025-02-05 19:43:37,508 - DEBUG - Images chechsum:   22,315,032, Outputs checksum:  -6.9981, Loss: 1.222133\n",
      "2025-02-05 19:44:14,591 - DEBUG - Epoch [ 2/10]: Train Loss: \u001b[91m1.4065\u001b[0m, Train Acc: 72.14%, Val Loss: \u001b[91m1.4744\u001b[0m, Val Acc: 60.00%, Time:  52 sec\n",
      "2025-02-05 19:44:24,541 - DEBUG - Images chechsum:   57,769,408, Outputs checksum: -24.3758, Loss: 1.252894\n",
      "2025-02-05 19:44:26,035 - DEBUG - Images chechsum:   15,081,962, Outputs checksum: -28.3450, Loss: 1.226033\n",
      "2025-02-05 19:44:27,506 - DEBUG - Images chechsum:   44,012,520, Outputs checksum: -21.0162, Loss: 1.164337\n",
      "2025-02-05 19:44:28,979 - DEBUG - Images chechsum:  105,083,656, Outputs checksum: -20.3858, Loss: 1.287519\n",
      "2025-02-05 19:44:29,549 - DEBUG - Images chechsum:   44,016,852, Outputs checksum: -10.0180, Loss: 1.044467\n",
      "2025-02-05 19:45:06,575 - DEBUG - Epoch [ 3/10]: Train Loss: \u001b[91m1.2166\u001b[0m, Train Acc: 74.29%, Val Loss: \u001b[91m1.3640\u001b[0m, Val Acc: 65.00%, Time:  52 sec\n",
      "2025-02-05 19:45:16,375 - DEBUG - Images chechsum:   -4,317,456, Outputs checksum: -21.9459, Loss: 1.129237\n",
      "2025-02-05 19:45:17,850 - DEBUG - Images chechsum:  108,442,728, Outputs checksum: -22.9429, Loss: 1.155578\n",
      "2025-02-05 19:45:19,349 - DEBUG - Images chechsum:   97,282,792, Outputs checksum: -25.0838, Loss: 0.930057\n",
      "2025-02-05 19:45:20,820 - DEBUG - Images chechsum:   75,521,968, Outputs checksum: -24.9921, Loss: 1.127588\n",
      "2025-02-05 19:45:21,392 - DEBUG - Images chechsum:   24,692,338, Outputs checksum: -16.4981, Loss: 0.923118\n",
      "2025-02-05 19:45:58,192 - DEBUG - Epoch [ 4/10]: Train Loss: \u001b[91m1.0717\u001b[0m, Train Acc: 80.00%, Val Loss: \u001b[91m1.2847\u001b[0m, Val Acc: 65.00%, Time:  52 sec\n",
      "2025-02-05 19:46:07,903 - DEBUG - Images chechsum:  172,908,192, Outputs checksum: -20.8294, Loss: 0.984789\n",
      "2025-02-05 19:46:09,386 - DEBUG - Images chechsum:  113,435,824, Outputs checksum: -34.0388, Loss: 0.984551\n",
      "2025-02-05 19:46:10,849 - DEBUG - Images chechsum:  138,261,824, Outputs checksum: -22.6832, Loss: 0.984116\n",
      "2025-02-05 19:46:12,310 - DEBUG - Images chechsum:  200,831,840, Outputs checksum: -32.3917, Loss: 0.993377\n",
      "2025-02-05 19:46:12,879 - DEBUG - Images chechsum:   22,348,710, Outputs checksum:  -9.9853, Loss: 1.055315\n",
      "2025-02-05 19:46:49,948 - DEBUG - Epoch [ 5/10]: Train Loss: \u001b[91m0.9926\u001b[0m, Train Acc: 79.29%, Val Loss: \u001b[91m1.2253\u001b[0m, Val Acc: 65.00%, Time:  52 sec\n",
      "2025-02-05 19:46:59,945 - DEBUG - Images chechsum:   93,153,600, Outputs checksum: -31.4745, Loss: 0.829567\n",
      "2025-02-05 19:47:01,449 - DEBUG - Images chechsum:  142,920,496, Outputs checksum: -27.8236, Loss: 0.939099\n",
      "2025-02-05 19:47:02,939 - DEBUG - Images chechsum:  -10,750,986, Outputs checksum: -25.1082, Loss: 1.091179\n",
      "2025-02-05 19:47:04,421 - DEBUG - Images chechsum:   93,637,656, Outputs checksum: -26.5739, Loss: 0.699647\n",
      "2025-02-05 19:47:04,987 - DEBUG - Images chechsum:   16,645,909, Outputs checksum: -12.3948, Loss: 0.833734\n",
      "2025-02-05 19:47:41,969 - DEBUG - Epoch [ 6/10]: Train Loss: \u001b[91m0.8851\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.1773\u001b[0m, Val Acc: 65.00%, Time:  52 sec\n",
      "2025-02-05 19:47:51,422 - DEBUG - Images chechsum:  256,731,712, Outputs checksum: -37.2278, Loss: 0.785617\n",
      "2025-02-05 19:47:52,772 - DEBUG - Images chechsum:  -16,570,136, Outputs checksum: -31.6605, Loss: 0.811595\n",
      "2025-02-05 19:47:54,044 - DEBUG - Images chechsum:   98,761,696, Outputs checksum: -32.3851, Loss: 0.912876\n",
      "2025-02-05 19:47:55,305 - DEBUG - Images chechsum:   33,314,864, Outputs checksum: -31.3605, Loss: 0.754954\n",
      "2025-02-05 19:47:55,787 - DEBUG - Images chechsum:   -6,805,710, Outputs checksum: -10.2954, Loss: 0.952120\n",
      "2025-02-05 19:48:32,452 - DEBUG - Epoch [ 7/10]: Train Loss: \u001b[91m0.8279\u001b[0m, Train Acc: 86.43%, Val Loss: \u001b[91m1.1408\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:48:41,670 - DEBUG - Images chechsum:   10,231,496, Outputs checksum: -35.4480, Loss: 0.754356\n",
      "2025-02-05 19:48:42,960 - DEBUG - Images chechsum:  162,842,656, Outputs checksum: -30.9745, Loss: 0.874266\n",
      "2025-02-05 19:48:44,237 - DEBUG - Images chechsum:   56,932,336, Outputs checksum: -36.7321, Loss: 0.794633\n",
      "2025-02-05 19:48:45,504 - DEBUG - Images chechsum:   45,655,088, Outputs checksum: -26.1418, Loss: 0.779366\n",
      "2025-02-05 19:48:45,988 - DEBUG - Images chechsum:   11,998,537, Outputs checksum:  -8.5046, Loss: 0.677677\n",
      "2025-02-05 19:49:22,077 - DEBUG - Epoch [ 8/10]: Train Loss: \u001b[91m0.7901\u001b[0m, Train Acc: 83.57%, Val Loss: \u001b[91m1.1096\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:49:31,556 - DEBUG - Images chechsum:   12,704,690, Outputs checksum: -27.1428, Loss: 0.805418\n",
      "2025-02-05 19:49:32,845 - DEBUG - Images chechsum:  190,852,528, Outputs checksum: -37.7655, Loss: 0.707214\n",
      "2025-02-05 19:49:34,126 - DEBUG - Images chechsum:   63,389,208, Outputs checksum: -33.8308, Loss: 0.679440\n",
      "2025-02-05 19:49:35,391 - DEBUG - Images chechsum:   48,731,196, Outputs checksum: -28.4260, Loss: 0.722415\n",
      "2025-02-05 19:49:35,878 - DEBUG - Images chechsum:   23,594,842, Outputs checksum: -13.4983, Loss: 0.697624\n",
      "2025-02-05 19:50:11,992 - DEBUG - Epoch [ 9/10]: Train Loss: \u001b[91m0.7260\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.0853\u001b[0m, Val Acc: 70.00%, Time:  50 sec\n",
      "2025-02-05 19:50:21,357 - DEBUG - Images chechsum:   59,281,200, Outputs checksum: -34.3334, Loss: 0.740532\n",
      "2025-02-05 19:50:22,624 - DEBUG - Images chechsum:   76,382,136, Outputs checksum: -39.7072, Loss: 0.620774\n",
      "2025-02-05 19:50:23,887 - DEBUG - Images chechsum:  -51,478,012, Outputs checksum: -29.2142, Loss: 0.710264\n",
      "2025-02-05 19:50:25,146 - DEBUG - Images chechsum:   75,953,576, Outputs checksum: -28.6153, Loss: 0.799852\n",
      "2025-02-05 19:50:25,633 - DEBUG - Images chechsum:   14,413,874, Outputs checksum:  -8.4555, Loss: 0.705677\n",
      "2025-02-05 19:51:01,998 - DEBUG - Epoch [10/10]: Train Loss: \u001b[91m0.7168\u001b[0m, Train Acc: 90.71%, Val Loss: \u001b[91m1.0653\u001b[0m, Val Acc: 70.00%, Time:  50 sec\n",
      "2025-02-05 19:51:21,000 - DEBUG - Trial: 1, Test Loss: 0.8105, Test Acc: 85.00%, Avg Time/Image: 0.008 sec\n",
      "2025-02-05 19:51:21,005 - INFO - Final Test: Loss: 0.8105, Acc: 85.00%\n",
      "2025-02-05 19:51:21,009 - INFO - Mean processing time per image: 0.008 sec\n",
      "2025-02-05 19:51:21,020 - DEBUG - Total cycle elapsed time:  8 min 55 sec\n",
      "2025-02-05 19:51:21,051 - DEBUG - \u001b[92mIteration 4\u001b[0m\n",
      "2025-02-05 19:51:21,060 - DEBUG - Using device: mps\n",
      "2025-02-05 19:51:21,060 - DEBUG - Config: {'BATCH_SIZE': 32, 'INITIAL_LR': 0.001, 'LR_DECOY': 0.9, 'NUM_EPOCHS': 10, 'DATA_DIR': '../Data/raw', 'IMAGE_SIZE': 242, 'NUM_CLASSES': 6, 'TEST_SPLIT_RATIO': 0.2, 'VALID_SPLIT_RATIO': 0.1, 'NORMALIZE': True, 'DEBUG': True, 'NUMBER_OF_TRIALS': 1, 'NUM_WORKER': 8, 'PREFETCH_FACTOR': 2}\n",
      "2025-02-05 19:51:21,081 - DEBUG - \u001b[92mNumber of workers: 8, Prefetch factor: 2, Batch size: 32\u001b[0m\n",
      "2025-02-05 19:51:21,082 - DEBUG - Training dataset indices: 4-2493, Number of instances: 140\n",
      "2025-02-05 19:51:21,082 - DEBUG - Validation dataset indices: 115-2484, Number of instances: 20\n",
      "2025-02-05 19:51:21,082 - DEBUG - Test dataset indices: 72-2526, Number of instances: 40\n",
      "2025-02-05 19:51:21,083 - DEBUG - Nun_workers: 8, prefetch_factor: 2\n",
      "2025-02-05 19:51:32,763 - DEBUG - Images chechsum:   96,798,416, Outputs checksum:  -5.7422, Loss: 1.778519\n",
      "2025-02-05 19:51:34,060 - DEBUG - Images chechsum:  108,718,832, Outputs checksum:  -1.7106, Loss: 1.741565\n",
      "2025-02-05 19:51:35,318 - DEBUG - Images chechsum:   77,164,784, Outputs checksum:  -8.2968, Loss: 1.693286\n",
      "2025-02-05 19:51:36,595 - DEBUG - Images chechsum:   56,238,016, Outputs checksum: -11.6934, Loss: 1.635638\n",
      "2025-02-05 19:51:37,084 - DEBUG - Images chechsum:  -21,610,722, Outputs checksum:  -4.9956, Loss: 1.621296\n",
      "2025-02-05 19:52:13,973 - DEBUG - Epoch [ 1/10]: Train Loss: \u001b[91m1.7045\u001b[0m, Train Acc: 27.86%, Val Loss: \u001b[91m1.6363\u001b[0m, Val Acc: 35.00%, Time:  51 sec\n",
      "2025-02-05 19:52:23,272 - DEBUG - Images chechsum:   84,747,416, Outputs checksum: -23.1232, Loss: 1.474313\n",
      "2025-02-05 19:52:24,560 - DEBUG - Images chechsum:  119,699,776, Outputs checksum: -16.2695, Loss: 1.391165\n",
      "2025-02-05 19:52:25,816 - DEBUG - Images chechsum:  174,519,712, Outputs checksum:  -9.6048, Loss: 1.436953\n",
      "2025-02-05 19:52:27,074 - DEBUG - Images chechsum:   44,154,360, Outputs checksum: -13.1407, Loss: 1.392747\n",
      "2025-02-05 19:52:27,560 - DEBUG - Images chechsum:   22,315,032, Outputs checksum:  -6.9981, Loss: 1.222133\n",
      "2025-02-05 19:53:04,521 - DEBUG - Epoch [ 2/10]: Train Loss: \u001b[91m1.4065\u001b[0m, Train Acc: 72.14%, Val Loss: \u001b[91m1.4744\u001b[0m, Val Acc: 60.00%, Time:  51 sec\n",
      "2025-02-05 19:53:13,826 - DEBUG - Images chechsum:   57,769,408, Outputs checksum: -24.3758, Loss: 1.252894\n",
      "2025-02-05 19:53:15,163 - DEBUG - Images chechsum:   15,081,962, Outputs checksum: -28.3450, Loss: 1.226033\n",
      "2025-02-05 19:53:16,426 - DEBUG - Images chechsum:   44,012,520, Outputs checksum: -21.0162, Loss: 1.164337\n",
      "2025-02-05 19:53:17,692 - DEBUG - Images chechsum:  105,083,656, Outputs checksum: -20.3858, Loss: 1.287519\n",
      "2025-02-05 19:53:18,178 - DEBUG - Images chechsum:   44,016,852, Outputs checksum: -10.0180, Loss: 1.044467\n",
      "2025-02-05 19:53:54,792 - DEBUG - Epoch [ 3/10]: Train Loss: \u001b[91m1.2166\u001b[0m, Train Acc: 74.29%, Val Loss: \u001b[91m1.3640\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:54:04,082 - DEBUG - Images chechsum:   -4,317,456, Outputs checksum: -21.9459, Loss: 1.129237\n",
      "2025-02-05 19:54:05,378 - DEBUG - Images chechsum:  108,442,728, Outputs checksum: -22.9429, Loss: 1.155578\n",
      "2025-02-05 19:54:06,638 - DEBUG - Images chechsum:   97,282,792, Outputs checksum: -25.0838, Loss: 0.930057\n",
      "2025-02-05 19:54:07,896 - DEBUG - Images chechsum:   75,521,968, Outputs checksum: -24.9921, Loss: 1.127588\n",
      "2025-02-05 19:54:08,379 - DEBUG - Images chechsum:   24,692,338, Outputs checksum: -16.4981, Loss: 0.923118\n",
      "2025-02-05 19:54:44,515 - DEBUG - Epoch [ 4/10]: Train Loss: \u001b[91m1.0717\u001b[0m, Train Acc: 80.00%, Val Loss: \u001b[91m1.2847\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:54:53,647 - DEBUG - Images chechsum:  172,908,192, Outputs checksum: -20.8294, Loss: 0.984789\n",
      "2025-02-05 19:54:54,936 - DEBUG - Images chechsum:  113,435,824, Outputs checksum: -34.0388, Loss: 0.984551\n",
      "2025-02-05 19:54:56,196 - DEBUG - Images chechsum:  138,261,824, Outputs checksum: -22.6832, Loss: 0.984116\n",
      "2025-02-05 19:54:57,458 - DEBUG - Images chechsum:  200,831,840, Outputs checksum: -32.3917, Loss: 0.993377\n",
      "2025-02-05 19:54:57,944 - DEBUG - Images chechsum:   22,348,710, Outputs checksum:  -9.9853, Loss: 1.055315\n",
      "2025-02-05 19:55:34,216 - DEBUG - Epoch [ 5/10]: Train Loss: \u001b[91m0.9926\u001b[0m, Train Acc: 79.29%, Val Loss: \u001b[91m1.2253\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:55:43,370 - DEBUG - Images chechsum:   93,153,600, Outputs checksum: -31.4745, Loss: 0.829567\n",
      "2025-02-05 19:55:44,652 - DEBUG - Images chechsum:  142,920,496, Outputs checksum: -27.8236, Loss: 0.939099\n",
      "2025-02-05 19:55:45,931 - DEBUG - Images chechsum:  -10,750,986, Outputs checksum: -25.1082, Loss: 1.091179\n",
      "2025-02-05 19:55:47,198 - DEBUG - Images chechsum:   93,637,656, Outputs checksum: -26.5739, Loss: 0.699647\n",
      "2025-02-05 19:55:47,687 - DEBUG - Images chechsum:   16,645,909, Outputs checksum: -12.3948, Loss: 0.833734\n",
      "2025-02-05 19:56:24,654 - DEBUG - Epoch [ 6/10]: Train Loss: \u001b[91m0.8851\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.1773\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:56:33,939 - DEBUG - Images chechsum:  256,731,712, Outputs checksum: -37.2278, Loss: 0.785617\n",
      "2025-02-05 19:56:35,219 - DEBUG - Images chechsum:  -16,570,136, Outputs checksum: -31.6605, Loss: 0.811595\n",
      "2025-02-05 19:56:36,484 - DEBUG - Images chechsum:   98,761,696, Outputs checksum: -32.3851, Loss: 0.912876\n",
      "2025-02-05 19:56:37,747 - DEBUG - Images chechsum:   33,314,864, Outputs checksum: -31.3605, Loss: 0.754954\n",
      "2025-02-05 19:56:38,231 - DEBUG - Images chechsum:   -6,805,710, Outputs checksum: -10.2954, Loss: 0.952120\n",
      "2025-02-05 19:57:14,682 - DEBUG - Epoch [ 7/10]: Train Loss: \u001b[91m0.8279\u001b[0m, Train Acc: 86.43%, Val Loss: \u001b[91m1.1408\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:57:23,882 - DEBUG - Images chechsum:   10,231,496, Outputs checksum: -35.4480, Loss: 0.754356\n",
      "2025-02-05 19:57:25,176 - DEBUG - Images chechsum:  162,842,656, Outputs checksum: -30.9745, Loss: 0.874266\n",
      "2025-02-05 19:57:26,429 - DEBUG - Images chechsum:   56,932,336, Outputs checksum: -36.7321, Loss: 0.794633\n",
      "2025-02-05 19:57:27,695 - DEBUG - Images chechsum:   45,655,088, Outputs checksum: -26.1418, Loss: 0.779366\n",
      "2025-02-05 19:57:28,184 - DEBUG - Images chechsum:   11,998,537, Outputs checksum:  -8.5046, Loss: 0.677677\n",
      "2025-02-05 19:58:04,815 - DEBUG - Epoch [ 8/10]: Train Loss: \u001b[91m0.7901\u001b[0m, Train Acc: 83.57%, Val Loss: \u001b[91m1.1096\u001b[0m, Val Acc: 65.00%, Time:  50 sec\n",
      "2025-02-05 19:58:14,239 - DEBUG - Images chechsum:   12,704,690, Outputs checksum: -27.1428, Loss: 0.805418\n",
      "2025-02-05 19:58:15,536 - DEBUG - Images chechsum:  190,852,528, Outputs checksum: -37.7655, Loss: 0.707214\n",
      "2025-02-05 19:58:16,798 - DEBUG - Images chechsum:   63,389,208, Outputs checksum: -33.8308, Loss: 0.679440\n",
      "2025-02-05 19:58:18,051 - DEBUG - Images chechsum:   48,731,196, Outputs checksum: -28.4260, Loss: 0.722415\n",
      "2025-02-05 19:58:18,551 - DEBUG - Images chechsum:   23,594,842, Outputs checksum: -13.4983, Loss: 0.697624\n",
      "2025-02-05 19:58:55,471 - DEBUG - Epoch [ 9/10]: Train Loss: \u001b[91m0.7260\u001b[0m, Train Acc: 88.57%, Val Loss: \u001b[91m1.0853\u001b[0m, Val Acc: 70.00%, Time:  51 sec\n",
      "2025-02-05 19:59:07,676 - DEBUG - Images chechsum:   59,281,200, Outputs checksum: -34.3334, Loss: 0.740532\n",
      "2025-02-05 19:59:09,041 - DEBUG - Images chechsum:   76,382,136, Outputs checksum: -39.7072, Loss: 0.620774\n",
      "2025-02-05 19:59:10,333 - DEBUG - Images chechsum:  -51,478,012, Outputs checksum: -29.2142, Loss: 0.710264\n",
      "2025-02-05 19:59:11,603 - DEBUG - Images chechsum:   75,953,576, Outputs checksum: -28.6153, Loss: 0.799852\n",
      "2025-02-05 19:59:12,094 - DEBUG - Images chechsum:   14,413,874, Outputs checksum:  -8.4555, Loss: 0.705677\n",
      "2025-02-05 19:59:48,889 - DEBUG - Epoch [10/10]: Train Loss: \u001b[91m0.7168\u001b[0m, Train Acc: 90.71%, Val Loss: \u001b[91m1.0653\u001b[0m, Val Acc: 70.00%, Time:  53 sec\n",
      "2025-02-05 20:00:07,823 - DEBUG - Trial: 1, Test Loss: 0.8105, Test Acc: 85.00%, Avg Time/Image: 0.007 sec\n",
      "2025-02-05 20:00:07,827 - INFO - Final Test: Loss: 0.8105, Acc: 85.00%\n",
      "2025-02-05 20:00:07,834 - INFO - Mean processing time per image: 0.007 sec\n",
      "2025-02-05 20:00:07,844 - DEBUG - Total cycle elapsed time:  8 min 47 sec\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_name = f\"../results_{time.strftime('%y%m%d_%H%M', time.localtime())}.csv\"\n",
    "    debug_summary = defaultdict(list)\n",
    "\n",
    "    # param_grid = [\n",
    "    #     {   \n",
    "    #         'BATCH_SIZE': 32, \n",
    "    #         'PREFETCH_FACTOR': [1, 2],\n",
    "    #         'INITIAL_LR': 0.004 \n",
    "    #     }, \n",
    "        # {   \n",
    "        #     'BATCH_SIZE': [40, 48], \n",
    "        #     'PREFETCH_FACTOR': [2, 4], \n",
    "        #     'INITIAL_LR': [0.004, 0.002, 0.001, 0.0005], \n",
    "        # }, \n",
    "\n",
    "    # ]\n",
    "    \n",
    "    # Generating all combinations\n",
    "    # for combination in get_param_grid(param_grid):\n",
    "    #     change_config(combination)\n",
    "\n",
    "    for i in range(4):\n",
    "        my_module_logger.debug(f'{GREEN}Iteration {i + 1:d}{RESET}')\n",
    "        cycle_start_time = time.time()\n",
    "\n",
    "        test_loss, test_acc, debug_data = main()\n",
    "        debug_summary[i + 1] = debug_data\n",
    "        \n",
    "        elapsed_time = (time.time() - cycle_start_time)\n",
    "        my_module_logger.debug(f'Total cycle elapsed time: {elapsed_time // 60:2.0f} min {elapsed_time % 60:2.0f} sec')\n",
    "        log_results(file_name, config, test_loss, test_acc, elapsed_time / 60)\n",
    "\n",
    "\n",
    "    # Convert debug_summary to flat DataFrame\n",
    "    df = flatten_defaultdict(debug_summary)\n",
    "    df = df[['Iteration',  'Epoch', 'Batch', 'Images_check', 'Outputs_check', 'Loss']].sort_values(by = ['Iteration', 'Epoch', 'Batch'])\n",
    "\n",
    "    # Calculate differencies for debuging\n",
    "    df_diff = df.copy()\n",
    "    df_diff['Images_check_diff'] = df.groupby(['Epoch', 'Batch'])['Images_check'].diff()\n",
    "    df_diff['Outputs_check_diff'] = df.groupby(['Epoch', 'Batch'])['Outputs_check'].diff()\n",
    "    df_diff['Loss_diff'] = df.groupby(['Epoch', 'Batch'])['Loss'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Iteration  Epoch  Batch  Images_check  Outputs_check      Loss  Images_check_diff  Outputs_check_diff  Loss_diff\n",
      "0            1      1      0    96798416.0      -5.742249  1.778519                NaN                 NaN        NaN\n",
      "1            1      1      1   108718832.0      -1.645323  1.741768                NaN                 NaN        NaN\n",
      "2            1      1      2    77164784.0      -8.356722  1.693612                NaN                 NaN        NaN\n",
      "3            1      1      3    56238016.0     -11.779107  1.635712                NaN                 NaN        NaN\n",
      "4            1      1      4   -21610722.0      -5.067947  1.621746                NaN                 NaN        NaN\n",
      "5            1      2      0    84747416.0     -23.493286  1.471361                NaN                 NaN        NaN\n",
      "6            1      2      1   119699776.0     -16.554253  1.390118                NaN                 NaN        NaN\n",
      "7            1      2      2   174519712.0      -9.807844  1.434784                NaN                 NaN        NaN\n",
      "8            1      2      3    44154360.0     -13.319701  1.390435                NaN                 NaN        NaN\n",
      "9            1      2      4    22315032.0      -7.102357  1.215767                NaN                 NaN        NaN\n",
      "10           1      3      0    57769408.0     -24.815887  1.245508                NaN                 NaN        NaN\n",
      "11           1      3      1    15081962.0     -28.789694  1.219098                NaN                 NaN        NaN\n",
      "12           1      3      2    44012520.0     -21.326876  1.157268                NaN                 NaN        NaN\n",
      "13           1      3      3   105083656.0     -20.665539  1.282876                NaN                 NaN        NaN\n",
      "14           1      3      4    44016852.0     -10.199151  1.033710                NaN                 NaN        NaN\n",
      "15           1      4      0    -4317456.0     -22.298008  1.119152                NaN                 NaN        NaN\n",
      "16           1      4      1   108442728.0     -23.355339  1.145063                NaN                 NaN        NaN\n",
      "17           1      4      2    97282792.0     -25.376202  0.916697                NaN                 NaN        NaN\n",
      "18           1      4      3    75521968.0     -25.509880  1.117010                NaN                 NaN        NaN\n",
      "19           1      4      4    24692338.0     -16.845598  0.906860                NaN                 NaN        NaN\n",
      "20           1      5      0   172908192.0     -21.094416  0.970764                NaN                 NaN        NaN\n",
      "21           1      5      1   113435824.0     -34.882168  0.971033                NaN                 NaN        NaN\n",
      "22           1      5      2   138261824.0     -23.048895  0.967060                NaN                 NaN        NaN\n",
      "23           1      5      3   200831840.0     -32.953869  0.979024                NaN                 NaN        NaN\n",
      "24           1      5      4    22348710.0     -10.197731  1.040490                NaN                 NaN        NaN\n",
      "25           1      6      0    93153600.0     -32.104874  0.811145                NaN                 NaN        NaN\n",
      "26           1      6      1   142920496.0     -28.366783  0.920377                NaN                 NaN        NaN\n",
      "27           1      6      2   -10750986.0     -25.557404  1.071934                NaN                 NaN        NaN\n",
      "28           1      6      3    93637656.0     -27.008213  0.679622                NaN                 NaN        NaN\n",
      "29           1      6      4    16645909.0     -12.681625  0.808829                NaN                 NaN        NaN\n",
      "30           1      7      0   256731712.0     -38.156536  0.762962                NaN                 NaN        NaN\n",
      "31           1      7      1   -16570136.0     -32.389908  0.794405                NaN                 NaN        NaN\n",
      "32           1      7      2    98761696.0     -32.908318  0.892960                NaN                 NaN        NaN\n",
      "33           1      7      3    33314864.0     -32.011955  0.731299                NaN                 NaN        NaN\n",
      "34           1      7      4    -6805710.5     -10.498041  0.926953                NaN                 NaN        NaN\n",
      "35           1      8      0    10231496.0     -36.136032  0.728612                NaN                 NaN        NaN\n",
      "36           1      8      1   162842656.0     -31.558022  0.848911                NaN                 NaN        NaN\n",
      "37           1      8      2    56932336.0     -37.642506  0.774005                NaN                 NaN        NaN\n",
      "38           1      8      3    45655088.0     -26.711134  0.756241                NaN                 NaN        NaN\n",
      "39           1      8      4    11998537.0      -8.643586  0.655916                NaN                 NaN        NaN\n",
      "40           1      9      0    12704690.0     -27.610054  0.778350                NaN                 NaN        NaN\n",
      "41           1      9      1   190852528.0     -38.826771  0.681196                NaN                 NaN        NaN\n",
      "42           1      9      2    63389208.0     -34.617523  0.654141                NaN                 NaN        NaN\n",
      "43           1      9      3    48731196.0     -29.003929  0.696909                NaN                 NaN        NaN\n",
      "44           1      9      4    23594842.0     -13.874645  0.668102                NaN                 NaN        NaN\n",
      "45           1     10      0    59281200.0     -35.103390  0.713422                NaN                 NaN        NaN\n",
      "46           1     10      1    76382136.0     -40.721329  0.590486                NaN                 NaN        NaN\n",
      "47           1     10      2   -51478012.0     -29.888439  0.683046                NaN                 NaN        NaN\n",
      "48           1     10      3    75953576.0     -29.280472  0.769206                NaN                 NaN        NaN\n",
      "49           1     10      4    14413874.0      -8.597281  0.674232                NaN                 NaN        NaN\n",
      "50           2      1      0    96798416.0      -5.742249  1.778519                0.0            0.000000   0.000000\n",
      "51           2      1      1   108718832.0      -1.710627  1.741565                0.0           -0.065304  -0.000203\n",
      "52           2      1      2    77164784.0      -8.296764  1.693286                0.0            0.059958  -0.000326\n",
      "53           2      1      3    56238016.0     -11.693390  1.635638                0.0            0.085717  -0.000074\n",
      "54           2      1      4   -21610722.0      -4.995642  1.621296                0.0            0.072306  -0.000450\n",
      "55           2      2      0    84747416.0     -23.123247  1.474313                0.0            0.370039   0.002952\n",
      "56           2      2      1   119699776.0     -16.269537  1.391165                0.0            0.284716   0.001047\n",
      "57           2      2      2   174519712.0      -9.604755  1.436953                0.0            0.203089   0.002169\n",
      "58           2      2      3    44154360.0     -13.140703  1.392747                0.0            0.178998   0.002312\n",
      "59           2      2      4    22315032.0      -6.998070  1.222133                0.0            0.104287   0.006366\n",
      "60           2      3      0    57769408.0     -24.375824  1.252894                0.0            0.440063   0.007386\n",
      "61           2      3      1    15081962.0     -28.345034  1.226033                0.0            0.444660   0.006934\n",
      "62           2      3      2    44012520.0     -21.016155  1.164337                0.0            0.310720   0.007070\n",
      "63           2      3      3   105083656.0     -20.385811  1.287519                0.0            0.279728   0.004643\n",
      "64           2      3      4    44016852.0     -10.018019  1.044467                0.0            0.181132   0.010757\n",
      "65           2      4      0    -4317456.0     -21.945877  1.129237                0.0            0.352131   0.010084\n",
      "66           2      4      1   108442728.0     -22.942913  1.155578                0.0            0.412426   0.010515\n",
      "67           2      4      2    97282792.0     -25.083765  0.930057                0.0            0.292437   0.013360\n",
      "68           2      4      3    75521968.0     -24.992075  1.127588                0.0            0.517805   0.010578\n",
      "69           2      4      4    24692338.0     -16.498070  0.923118                0.0            0.347528   0.016258\n",
      "70           2      5      0   172908192.0     -20.829412  0.984789                0.0            0.265003   0.014025\n",
      "71           2      5      1   113435824.0     -34.038815  0.984551                0.0            0.843353   0.013518\n",
      "72           2      5      2   138261824.0     -22.683182  0.984116                0.0            0.365713   0.017056\n",
      "73           2      5      3   200831840.0     -32.391678  0.993377                0.0            0.562191   0.014353\n",
      "74           2      5      4    22348710.0      -9.985281  1.055315                0.0            0.212450   0.014826\n",
      "75           2      6      0    93153600.0     -31.474463  0.829567                0.0            0.630411   0.018422\n",
      "76           2      6      1   142920496.0     -27.823622  0.939099                0.0            0.543161   0.018722\n",
      "77           2      6      2   -10750986.0     -25.108221  1.091179                0.0            0.449183   0.019245\n",
      "78           2      6      3    93637656.0     -26.573935  0.699647                0.0            0.434278   0.020026\n",
      "79           2      6      4    16645909.0     -12.394774  0.833734                0.0            0.286851   0.024905\n",
      "80           2      7      0   256731712.0     -37.227840  0.785617                0.0            0.928696   0.022655\n",
      "81           2      7      1   -16570136.0     -31.660465  0.811595                0.0            0.729443   0.017189\n",
      "82           2      7      2    98761696.0     -32.385136  0.912876                0.0            0.523182   0.019916\n",
      "83           2      7      3    33314864.0     -31.360550  0.754954                0.0            0.651405   0.023654\n",
      "84           2      7      4    -6805710.5     -10.295446  0.952120                0.0            0.202595   0.025168\n",
      "85           2      8      0    10231496.0     -35.448009  0.754356                0.0            0.688023   0.025744\n",
      "86           2      8      1   162842656.0     -30.974497  0.874266                0.0            0.583525   0.025355\n",
      "87           2      8      2    56932336.0     -36.732121  0.794633                0.0            0.910385   0.020628\n",
      "88           2      8      3    45655088.0     -26.141787  0.779366                0.0            0.569347   0.023125\n",
      "89           2      8      4    11998537.0      -8.504644  0.677677                0.0            0.138942   0.021761\n",
      "90           2      9      0    12704690.0     -27.142822  0.805418                0.0            0.467232   0.027068\n",
      "91           2      9      1   190852528.0     -37.765495  0.707214                0.0            1.061275   0.026019\n",
      "92           2      9      2    63389208.0     -33.830849  0.679440                0.0            0.786674   0.025299\n",
      "93           2      9      3    48731196.0     -28.425991  0.722415                0.0            0.577938   0.025505\n",
      "94           2      9      4    23594842.0     -13.498259  0.697624                0.0            0.376387   0.029522\n",
      "95           2     10      0    59281200.0     -34.333420  0.740532                0.0            0.769970   0.027110\n",
      "96           2     10      1    76382136.0     -39.707188  0.620774                0.0            1.014141   0.030288\n",
      "97           2     10      2   -51478012.0     -29.214237  0.710264                0.0            0.674202   0.027217\n",
      "98           2     10      3    75953576.0     -28.615269  0.799852                0.0            0.665203   0.030647\n",
      "99           2     10      4    14413874.0      -8.455455  0.705677                0.0            0.141826   0.031445\n",
      "100          3      1      0    96798416.0      -5.742249  1.778519                0.0            0.000000   0.000000\n",
      "101          3      1      1   108718832.0      -1.710627  1.741565                0.0            0.000000   0.000000\n",
      "102          3      1      2    77164784.0      -8.296764  1.693286                0.0            0.000000   0.000000\n",
      "103          3      1      3    56238016.0     -11.693390  1.635638                0.0            0.000000   0.000000\n",
      "104          3      1      4   -21610722.0      -4.995642  1.621296                0.0            0.000000   0.000000\n",
      "105          3      2      0    84747416.0     -23.123247  1.474313                0.0            0.000000   0.000000\n",
      "106          3      2      1   119699776.0     -16.269537  1.391165                0.0            0.000000   0.000000\n",
      "107          3      2      2   174519712.0      -9.604755  1.436953                0.0            0.000000   0.000000\n",
      "108          3      2      3    44154360.0     -13.140703  1.392747                0.0            0.000000   0.000000\n",
      "109          3      2      4    22315032.0      -6.998070  1.222133                0.0            0.000000   0.000000\n",
      "110          3      3      0    57769408.0     -24.375824  1.252894                0.0            0.000000   0.000000\n",
      "111          3      3      1    15081962.0     -28.345034  1.226033                0.0            0.000000   0.000000\n",
      "112          3      3      2    44012520.0     -21.016155  1.164337                0.0            0.000000   0.000000\n",
      "113          3      3      3   105083656.0     -20.385811  1.287519                0.0            0.000000   0.000000\n",
      "114          3      3      4    44016852.0     -10.018019  1.044467                0.0            0.000000   0.000000\n",
      "115          3      4      0    -4317456.0     -21.945877  1.129237                0.0            0.000000   0.000000\n",
      "116          3      4      1   108442728.0     -22.942913  1.155578                0.0            0.000000   0.000000\n",
      "117          3      4      2    97282792.0     -25.083765  0.930057                0.0            0.000000   0.000000\n",
      "118          3      4      3    75521968.0     -24.992075  1.127588                0.0            0.000000   0.000000\n",
      "119          3      4      4    24692338.0     -16.498070  0.923118                0.0            0.000000   0.000000\n",
      "120          3      5      0   172908192.0     -20.829412  0.984789                0.0            0.000000   0.000000\n",
      "121          3      5      1   113435824.0     -34.038815  0.984551                0.0            0.000000   0.000000\n",
      "122          3      5      2   138261824.0     -22.683182  0.984116                0.0            0.000000   0.000000\n",
      "123          3      5      3   200831840.0     -32.391678  0.993377                0.0            0.000000   0.000000\n",
      "124          3      5      4    22348710.0      -9.985281  1.055315                0.0            0.000000   0.000000\n",
      "125          3      6      0    93153600.0     -31.474463  0.829567                0.0            0.000000   0.000000\n",
      "126          3      6      1   142920496.0     -27.823622  0.939099                0.0            0.000000   0.000000\n",
      "127          3      6      2   -10750986.0     -25.108221  1.091179                0.0            0.000000   0.000000\n",
      "128          3      6      3    93637656.0     -26.573935  0.699647                0.0            0.000000   0.000000\n",
      "129          3      6      4    16645909.0     -12.394774  0.833734                0.0            0.000000   0.000000\n",
      "130          3      7      0   256731712.0     -37.227840  0.785617                0.0            0.000000   0.000000\n",
      "131          3      7      1   -16570136.0     -31.660465  0.811595                0.0            0.000000   0.000000\n",
      "132          3      7      2    98761696.0     -32.385136  0.912876                0.0            0.000000   0.000000\n",
      "133          3      7      3    33314864.0     -31.360550  0.754954                0.0            0.000000   0.000000\n",
      "134          3      7      4    -6805710.5     -10.295446  0.952120                0.0            0.000000   0.000000\n",
      "135          3      8      0    10231496.0     -35.448009  0.754356                0.0            0.000000   0.000000\n",
      "136          3      8      1   162842656.0     -30.974497  0.874266                0.0            0.000000   0.000000\n",
      "137          3      8      2    56932336.0     -36.732121  0.794633                0.0            0.000000   0.000000\n",
      "138          3      8      3    45655088.0     -26.141787  0.779366                0.0            0.000000   0.000000\n",
      "139          3      8      4    11998537.0      -8.504644  0.677677                0.0            0.000000   0.000000\n",
      "140          3      9      0    12704690.0     -27.142822  0.805418                0.0            0.000000   0.000000\n",
      "141          3      9      1   190852528.0     -37.765495  0.707214                0.0            0.000000   0.000000\n",
      "142          3      9      2    63389208.0     -33.830849  0.679440                0.0            0.000000   0.000000\n",
      "143          3      9      3    48731196.0     -28.425991  0.722415                0.0            0.000000   0.000000\n",
      "144          3      9      4    23594842.0     -13.498259  0.697624                0.0            0.000000   0.000000\n",
      "145          3     10      0    59281200.0     -34.333420  0.740532                0.0            0.000000   0.000000\n",
      "146          3     10      1    76382136.0     -39.707188  0.620774                0.0            0.000000   0.000000\n",
      "147          3     10      2   -51478012.0     -29.214237  0.710264                0.0            0.000000   0.000000\n",
      "148          3     10      3    75953576.0     -28.615269  0.799852                0.0            0.000000   0.000000\n",
      "149          3     10      4    14413874.0      -8.455455  0.705677                0.0            0.000000   0.000000\n",
      "150          4      1      0    96798416.0      -5.742249  1.778519                0.0            0.000000   0.000000\n",
      "151          4      1      1   108718832.0      -1.710627  1.741565                0.0            0.000000   0.000000\n",
      "152          4      1      2    77164784.0      -8.296764  1.693286                0.0            0.000000   0.000000\n",
      "153          4      1      3    56238016.0     -11.693390  1.635638                0.0            0.000000   0.000000\n",
      "154          4      1      4   -21610722.0      -4.995642  1.621296                0.0            0.000000   0.000000\n",
      "155          4      2      0    84747416.0     -23.123247  1.474313                0.0            0.000000   0.000000\n",
      "156          4      2      1   119699776.0     -16.269537  1.391165                0.0            0.000000   0.000000\n",
      "157          4      2      2   174519712.0      -9.604755  1.436953                0.0            0.000000   0.000000\n",
      "158          4      2      3    44154360.0     -13.140703  1.392747                0.0            0.000000   0.000000\n",
      "159          4      2      4    22315032.0      -6.998070  1.222133                0.0            0.000000   0.000000\n",
      "160          4      3      0    57769408.0     -24.375824  1.252894                0.0            0.000000   0.000000\n",
      "161          4      3      1    15081962.0     -28.345034  1.226033                0.0            0.000000   0.000000\n",
      "162          4      3      2    44012520.0     -21.016155  1.164337                0.0            0.000000   0.000000\n",
      "163          4      3      3   105083656.0     -20.385811  1.287519                0.0            0.000000   0.000000\n",
      "164          4      3      4    44016852.0     -10.018019  1.044467                0.0            0.000000   0.000000\n",
      "165          4      4      0    -4317456.0     -21.945877  1.129237                0.0            0.000000   0.000000\n",
      "166          4      4      1   108442728.0     -22.942913  1.155578                0.0            0.000000   0.000000\n",
      "167          4      4      2    97282792.0     -25.083765  0.930057                0.0            0.000000   0.000000\n",
      "168          4      4      3    75521968.0     -24.992075  1.127588                0.0            0.000000   0.000000\n",
      "169          4      4      4    24692338.0     -16.498070  0.923118                0.0            0.000000   0.000000\n",
      "170          4      5      0   172908192.0     -20.829412  0.984789                0.0            0.000000   0.000000\n",
      "171          4      5      1   113435824.0     -34.038815  0.984551                0.0            0.000000   0.000000\n",
      "172          4      5      2   138261824.0     -22.683182  0.984116                0.0            0.000000   0.000000\n",
      "173          4      5      3   200831840.0     -32.391678  0.993377                0.0            0.000000   0.000000\n",
      "174          4      5      4    22348710.0      -9.985281  1.055315                0.0            0.000000   0.000000\n",
      "175          4      6      0    93153600.0     -31.474463  0.829567                0.0            0.000000   0.000000\n",
      "176          4      6      1   142920496.0     -27.823622  0.939099                0.0            0.000000   0.000000\n",
      "177          4      6      2   -10750986.0     -25.108221  1.091179                0.0            0.000000   0.000000\n",
      "178          4      6      3    93637656.0     -26.573935  0.699647                0.0            0.000000   0.000000\n",
      "179          4      6      4    16645909.0     -12.394774  0.833734                0.0            0.000000   0.000000\n",
      "180          4      7      0   256731712.0     -37.227840  0.785617                0.0            0.000000   0.000000\n",
      "181          4      7      1   -16570136.0     -31.660465  0.811595                0.0            0.000000   0.000000\n",
      "182          4      7      2    98761696.0     -32.385136  0.912876                0.0            0.000000   0.000000\n",
      "183          4      7      3    33314864.0     -31.360550  0.754954                0.0            0.000000   0.000000\n",
      "184          4      7      4    -6805710.5     -10.295446  0.952120                0.0            0.000000   0.000000\n",
      "185          4      8      0    10231496.0     -35.448009  0.754356                0.0            0.000000   0.000000\n",
      "186          4      8      1   162842656.0     -30.974497  0.874266                0.0            0.000000   0.000000\n",
      "187          4      8      2    56932336.0     -36.732121  0.794633                0.0            0.000000   0.000000\n",
      "188          4      8      3    45655088.0     -26.141787  0.779366                0.0            0.000000   0.000000\n",
      "189          4      8      4    11998537.0      -8.504644  0.677677                0.0            0.000000   0.000000\n",
      "190          4      9      0    12704690.0     -27.142822  0.805418                0.0            0.000000   0.000000\n",
      "191          4      9      1   190852528.0     -37.765495  0.707214                0.0            0.000000   0.000000\n",
      "192          4      9      2    63389208.0     -33.830849  0.679440                0.0            0.000000   0.000000\n",
      "193          4      9      3    48731196.0     -28.425991  0.722415                0.0            0.000000   0.000000\n",
      "194          4      9      4    23594842.0     -13.498259  0.697624                0.0            0.000000   0.000000\n",
      "195          4     10      0    59281200.0     -34.333420  0.740532                0.0            0.000000   0.000000\n",
      "196          4     10      1    76382136.0     -39.707188  0.620774                0.0            0.000000   0.000000\n",
      "197          4     10      2   -51478012.0     -29.214237  0.710264                0.0            0.000000   0.000000\n",
      "198          4     10      3    75953576.0     -28.615269  0.799852                0.0            0.000000   0.000000\n",
      "199          4     10      4    14413874.0      -8.455455  0.705677                0.0            0.000000   0.000000\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.width', 200, 'display.max_rows', 200):\n",
    "    print(df_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
